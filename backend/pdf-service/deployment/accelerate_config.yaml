# Accelerate Configuration for NEXA
# Supports both CPU and GPU environments

compute_environment: LOCAL_MACHINE
debug: false
distributed_type: 'NO'  # Can be changed to 'MULTI_GPU' or 'TPU' for distributed training
downcast_bf16: 'no'
gpu_ids: all  # Use all available GPUs
machine_rank: 0
main_training_function: main
mixed_precision: 'no'  # Can be 'fp16' or 'bf16' for mixed precision
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false  # Set to true to force CPU usage

# Additional settings for optimization
gradient_accumulation_steps: 1
gradient_clipping: 1.0
zero_stage: 0  # DeepSpeed ZeRO optimization stage (0, 1, 2, or 3)

# Memory management
offload_optimizer_device: none  # Can be 'cpu' to offload optimizer states
offload_param_device: none  # Can be 'cpu' to offload parameters
