#!/usr/bin/env python3
"""
Enhanced Spec Analyzer with Fine-Tuned NER for Poles & Underground
Integrates with existing app_oct2025_enhanced.py
"""

import os
import pickle
import json
import torch
import numpy as np
from typing import List, Dict, Any, Tuple
from pathlib import Path
from sentence_transformers import SentenceTransformer, util
from transformers import AutoModelForTokenClassification, AutoTokenizer
import logging
from datetime import datetime
import hashlib

logger = logging.getLogger(__name__)

class EnhancedSpecAnalyzer:
    """
    Production-ready analyzer with fine-tuned domain knowledge
    """
    
    def __init__(self, 
                 spec_embeddings_path="/data/spec_embeddings.pkl",
                 fine_tuned_model_path="/data/fine_tuned_ner",
                 confidence_threshold=0.85):
        """
        Initialize enhanced analyzer
        
        Args:
            spec_embeddings_path: Path to stored spec embeddings
            fine_tuned_model_path: Path to fine-tuned NER model
            confidence_threshold: Threshold for high-confidence repeals
        """
        
        self.spec_embeddings_path = spec_embeddings_path
        self.confidence_threshold = confidence_threshold
        
        # Load embeddings model
        logger.info("Loading sentence transformer...")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Load fine-tuned NER model if available
        self.ner_model = None
        self.tokenizer = None
        if os.path.exists(fine_tuned_model_path):
            logger.info(f"Loading fine-tuned model from {fine_tuned_model_path}")
            self.ner_model = AutoModelForTokenClassification.from_pretrained(fine_tuned_model_path)
            self.tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)
            logger.info("âœ… Fine-tuned NER model loaded")
        else:
            logger.warning("âš ï¸ Fine-tuned model not found, using base embeddings only")
        
        # Load spec embeddings
        self.spec_chunks = []
        self.spec_embeddings = None
        self.load_spec_embeddings()
        
        # Statistics tracking
        self.stats = {
            "total_infractions_analyzed": 0,
            "repealable_count": 0,
            "review_required_count": 0,
            "valid_infraction_count": 0,
            "avg_confidence": 0.0
        }
    
    def load_spec_embeddings(self):
        """Load pre-computed spec embeddings"""
        
        if os.path.exists(self.spec_embeddings_path):
            try:
                with open(self.spec_embeddings_path, 'rb') as f:
                    data = pickle.load(f)
                    self.spec_chunks = data.get('chunks', [])
                    self.spec_embeddings = data.get('embeddings', None)
                logger.info(f"âœ… Loaded {len(self.spec_chunks)} spec chunks")
            except Exception as e:
                logger.error(f"Failed to load spec embeddings: {e}")
                self.spec_chunks = []
                self.spec_embeddings = None
        else:
            logger.warning(f"Spec embeddings not found at {self.spec_embeddings_path}")
    
    def analyze_go_back_infraction(self, infraction_text: str) -> Dict[str, Any]:
        """
        Analyze a go-back infraction for poles/underground equipment
        
        Args:
            infraction_text: Text describing the infraction
            
        Returns:
            Analysis result with confidence, status, and reasoning
        """
        
        self.stats["total_infractions_analyzed"] += 1
        
        # Extract domain-specific entities if NER available
        entities = self.extract_entities(infraction_text) if self.ner_model else []
        
        # Generate embedding for infraction
        infraction_embedding = self.embedder.encode(infraction_text, convert_to_tensor=True)
        
        # Find best matching specs
        matches = self.find_best_spec_matches(infraction_embedding, top_k=5)
        
        # Calculate enhanced confidence with entity bonus
        confidence, reasoning = self.calculate_confidence_with_reasoning(
            infraction_text, 
            matches, 
            entities
        )
        
        # Determine status based on confidence
        status = self.determine_status(confidence)
        
        # Update stats
        if status == "REPEALABLE":
            self.stats["repealable_count"] += 1
        elif status == "REVIEW_REQUIRED":
            self.stats["review_required_count"] += 1
        else:
            self.stats["valid_infraction_count"] += 1
        
        # Calculate running average confidence
        n = self.stats["total_infractions_analyzed"]
        self.stats["avg_confidence"] = (
            (self.stats["avg_confidence"] * (n - 1) + confidence) / n
        )
        
        # Check for specific pole/underground keywords
        equipment_type = self.identify_equipment_type(infraction_text, entities)
        
        return {
            "infraction_id": hashlib.md5(infraction_text.encode()).hexdigest()[:8],
            "status": status,
            "confidence": round(confidence, 3),
            "confidence_percentage": f"{confidence * 100:.1f}%",
            "equipment_type": equipment_type,
            "entities_detected": entities,
            "matched_specs": [
                {
                    "spec_text": match["text"][:200],
                    "similarity": round(match["score"], 3),
                    "source": match.get("source", "Unknown")
                }
                for match in matches[:3]  # Top 3 matches
            ],
            "reasoning": reasoning,
            "recommendation": self.generate_recommendation(status, confidence, equipment_type),
            "timestamp": datetime.now().isoformat()
        }
    
    def extract_entities(self, text: str) -> List[Dict]:
        """Extract entities using fine-tuned NER model (F1=0.87)"""
        
        entities = []
        
        # Try to use global fine-tuned NER pipeline first
        try:
            import sys
            import os
            # Add parent directory to path to import from app
            sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
            
            # Check if NER pipeline exists in global scope
            if 'ner_pipeline' in globals() and globals()['ner_pipeline'] is not None:
                # Use fine-tuned NER pipeline (F1=0.87)
                ner_results = globals()['ner_pipeline'](text)
                
                for result in ner_results:
                    entities.append({
                        "text": result["word"],
                        "label": result["entity_group"],
                        "score": result["score"]
                    })
                
                logger.info(f"Fine-tuned NER (F1=0.87) extracted {len(entities)} entities")
                return entities
                
        except Exception as e:
            logger.debug(f"Could not use global NER pipeline: {e}")
        
        # Fallback to local NER model if available
        if self.ner_model and self.tokenizer:
            try:
                inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512
                )
                
                with torch.no_grad():
                    outputs = self.ner_model(**inputs)
                
                # Process outputs properly
                predictions = torch.argmax(outputs.logits, dim=-1)
                tokens = self.tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
                
                # Basic entity extraction from predictions
                current_entity = None
                for token, pred in zip(tokens, predictions[0]):
                    if pred > 0:  # Non-O tag
                        if current_entity is None:
                            current_entity = {"text": token, "label": str(pred.item())}
                        else:
                            current_entity["text"] += token.replace("##", "")
                    elif current_entity:
                        entities.append(current_entity)
                        current_entity = None
                
                logger.info(f"Local NER model extracted {len(entities)} entities")
                return entities
                
            except Exception as e:
                logger.error(f"Local NER extraction failed: {e}")
        
        # Final fallback to pattern matching
        logger.info("Using pattern matching for entity extraction")
        return self.pattern_extract_entities(text)
    
    def find_best_spec_matches(self, query_embedding, top_k=5) -> List[Dict]:
        """Find best matching spec chunks"""
        
        if self.spec_embeddings is None or len(self.spec_chunks) == 0:
            return []
        
        # Calculate cosine similarities
        similarities = util.cos_sim(query_embedding, self.spec_embeddings)[0]
        
        # Get top-k matches
        top_results = torch.topk(similarities, k=min(top_k, len(self.spec_chunks)))
        
        matches = []
        for score, idx in zip(top_results.values, top_results.indices):
            matches.append({
                "text": self.spec_chunks[idx.item()],
                "score": score.item(),
                "source": self.extract_source_from_chunk(self.spec_chunks[idx.item()])
            })
        
        return matches
    
    def extract_source_from_chunk(self, chunk: str) -> str:
        """Extract source document from chunk text"""
        
        # Look for patterns like [Source: ...] or Document XXX
        import re
        
        source_match = re.search(r'\[Source: ([^\]]+)\]', chunk)
        if source_match:
            return source_match.group(1)
        
        doc_match = re.search(r'Document\s+(\d+)', chunk)
        if doc_match:
            return f"Document {doc_match.group(1)}"
        
        standard_match = re.search(r'(G\.O\.\s+\d+|ANSI\s+[A-Z0-9.]+|EMS-\d+)', chunk)
        if standard_match:
            return standard_match.group(1)
        
        return "PG&E Specification"
    
    def calculate_confidence_with_reasoning(self, 
                                           infraction_text: str,
                                           matches: List[Dict],
                                           entities: List[Dict]) -> Tuple[float, str]:
        """Calculate confidence with detailed reasoning"""
        
        if not matches:
            return 0.0, "No matching specifications found"
        
        # Base confidence from similarity
        base_confidence = matches[0]["score"]
        
        # Entity-based bonuses
        entity_bonus = 0.0
        critical_entities = ["MEASURE", "STANDARD", "SPECIFICATION", "EQUIPMENT"]
        entity_matches = []
        
        for entity in entities:
            if entity["label"] in critical_entities:
                # Check if entity appears in top match
                if entity["text"].lower() in matches[0]["text"].lower():
                    entity_bonus += 0.05
                    entity_matches.append(f"{entity['label']}: {entity['text']}")
        
        # Keyword bonuses for poles/underground
        keyword_bonus = 0.0
        pole_keywords = ["pole", "clearance", "feet", "crossarm", "weatherhead", "attachment"]
        underground_keywords = ["conduit", "trench", "buried", "cover", "depth", "underground"]
        
        infraction_lower = infraction_text.lower()
        for keyword in pole_keywords + underground_keywords:
            if keyword in infraction_lower and keyword in matches[0]["text"].lower():
                keyword_bonus += 0.02
        
        # Calculate final confidence
        final_confidence = min(base_confidence + entity_bonus + keyword_bonus, 1.0)
        
        # Generate reasoning
        if final_confidence >= self.confidence_threshold:
            reasoning = f"Infraction false ({final_confidence*100:.0f}% confidence) - "
            reasoning += f"Repealed per {matches[0]['source']}. "
            if entity_matches:
                reasoning += f"Matched entities: {', '.join(entity_matches[:3])}."
        elif final_confidence >= 0.70:
            reasoning = f"Review required ({final_confidence*100:.0f}% confidence) - "
            reasoning += f"Partial match with {matches[0]['source']}. "
            reasoning += "Manual verification recommended."
        else:
            reasoning = f"Valid infraction ({final_confidence*100:.0f}% confidence) - "
            reasoning += "No clear specification match found."
        
        return final_confidence, reasoning
    
    def determine_status(self, confidence: float) -> str:
        """Determine infraction status based on confidence"""
        
        if confidence >= self.confidence_threshold:
            return "REPEALABLE"
        elif confidence >= 0.70:
            return "REVIEW_REQUIRED"
        else:
            return "VALID_INFRACTION"
    
    def identify_equipment_type(self, text: str, entities: List[Dict]) -> str:
        """Identify whether infraction relates to poles or underground"""
        
        text_lower = text.lower()
        
        # Check entities first
        for entity in entities:
            if entity["label"] == "EQUIPMENT":
                if "pole" in entity["text"].lower():
                    return "OVERHEAD_POLE"
                elif any(word in entity["text"].lower() for word in ["conduit", "trench", "transformer"]):
                    return "UNDERGROUND"
        
        # Fallback to keyword detection
        pole_score = sum(1 for word in ["pole", "crossarm", "overhead", "clearance", "attachment"] 
                         if word in text_lower)
        underground_score = sum(1 for word in ["underground", "conduit", "trench", "buried", "depth"] 
                               if word in text_lower)
        
        if pole_score > underground_score:
            return "OVERHEAD_POLE"
        elif underground_score > pole_score:
            return "UNDERGROUND"
        else:
            return "GENERAL"
    
    def generate_recommendation(self, status: str, confidence: float, equipment_type: str) -> str:
        """Generate actionable recommendation"""
        
        if status == "REPEALABLE":
            return f"âœ… Auto-approve repeal. {equipment_type} specs clearly support field conditions."
        elif status == "REVIEW_REQUIRED":
            return f"âš ï¸ QA review needed. Check {equipment_type} measurements against spec requirements."
        else:
            return f"âŒ Infraction stands. Schedule correction for {equipment_type} issue."
    
    def get_statistics(self) -> Dict:
        """Get analyzer statistics"""
        
        return {
            **self.stats,
            "confidence_threshold": self.confidence_threshold,
            "model_type": "fine-tuned" if self.ner_model else "base",
            "spec_chunks_loaded": len(self.spec_chunks),
            "repeal_rate": (
                self.stats["repealable_count"] / self.stats["total_infractions_analyzed"] * 100
                if self.stats["total_infractions_analyzed"] > 0 else 0
            )
        }

def integrate_with_app(app):
    """
    Add enhanced analyzer endpoints to existing FastAPI app
    """
    from fastapi import UploadFile, File, HTTPException
    from typing import List
    
    analyzer = EnhancedSpecAnalyzer()
    
    @app.post("/analyze-go-back")
    async def analyze_go_back(
        infraction_text: str,
        confidence_threshold: float = 0.85
    ):
        """
        Analyze a go-back infraction with enhanced NER
        
        Returns confidence score, repeal status, and reasoning
        """
        try:
            analyzer.confidence_threshold = confidence_threshold
            result = analyzer.analyze_go_back_infraction(infraction_text)
            return result
        except Exception as e:
            logger.error(f"Analysis failed: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.post("/batch-analyze-go-backs")
    async def batch_analyze_go_backs(infractions: List[str]):
        """Analyze multiple go-back infractions"""
        
        results = []
        for infraction in infractions:
            try:
                result = analyzer.analyze_go_back_infraction(infraction)
                results.append(result)
            except Exception as e:
                results.append({
                    "error": str(e),
                    "infraction": infraction[:100]
                })
        
        return {
            "results": results,
            "statistics": analyzer.get_statistics()
        }
    
    @app.get("/analyzer-stats")
    async def get_analyzer_statistics():
        """Get performance statistics for the analyzer"""
        return analyzer.get_statistics()
    
    logger.info("âœ… Enhanced analyzer endpoints added to app")

if __name__ == "__main__":
    # Test the enhanced analyzer
    analyzer = EnhancedSpecAnalyzer()
    
    test_infractions = [
        "Pole clearance violation: measured 15 feet over street, requires 18 feet minimum per G.O. 95",
        "Underground conduit depth only 20 inches for primary service, specification requires 30 inches",
        "Crossarm attachment point 24 inches from pole top, weatherhead distance non-compliant",
        "Metal pole installation without climbing provisions, safety violation",
        "Transformer pad clearance 5 feet front, vegetation encroachment detected"
    ]
    
    print("\nðŸ§ª Testing Enhanced Analyzer:\n")
    for infraction in test_infractions:
        result = analyzer.analyze_go_back_infraction(infraction)
        print(f"Infraction: {infraction[:80]}...")
        print(f"Status: {result['status']} ({result['confidence_percentage']})")
        print(f"Type: {result['equipment_type']}")
        print(f"Reasoning: {result['reasoning']}")
        print(f"Recommendation: {result['recommendation']}\n")
    
    print("\nðŸ“Š Statistics:")
    stats = analyzer.get_statistics()
    for key, value in stats.items():
        print(f"  {key}: {value}")
